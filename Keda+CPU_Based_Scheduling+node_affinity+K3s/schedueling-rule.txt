

1. Boot sequence / “traffic light”

* When you run build.py, KEDA should be the traffic light that **creates exactly 10 work items (pods)** from your 10 analyzer images.
* Those pods aren’t allowed to start immediately—they’re **queued** (i.e., held back so nothing runs yet).

2. Decide how many pods may run now

* KEDA **scrapes Prometheus for 30 seconds**, computes the **average CPU usage across all nodes**, and figures out **which nodes are “available”** (under your CPU threshold).
* If ≥1 node is available, KEDA **releases exactly one pod per available node** (so if 3 nodes are under threshold, 3 pods start running; the rest remain in the queue).

3. Place each released pod on the “best” node

* Scheduling should ensure **one pod per node**.
* Pods should land on the **least-loaded nodes first** (by nodeAffinity/selector logic).
* If multiple nodes tie (e.g., same or 0% CPU), place them **randomly** but still maintain **one pod per node**.

4. Drip-feed until all 10 are done

* As soon as a running pod **completes**, KEDA **releases one more** from the queue (keeping concurrency ≈ number of available nodes).
* Repeat this pattern until **all 10 pods have run to completion**.

5. Periodic overload protection (every 15 minutes)

* Independently of the 30-second admission checks, every **15 minutes** KEDA scrapes Prometheus and computes the **15-minute average CPU per node**.
* If **any node is overloaded** by that 15-min average, the pod running there is **deleted/evicted**, that work item goes **back to the queue**, and it will be **re-released later** to the least-loaded node (again maintaining one pod per node).

Slow overload loop (15-minute avg) — KEDA emits the signal; controller evicts specific pods


(External Scaler – cleanest “KEDA signals”):
Write a tiny External Scaler service (gRPC) that KEDA calls.

It computes a 15-minute overload metric (e.g., overloaded_nodes_count) from Prometheus.

When > 0, the scaler POSTs a small JSON to your controller (or writes a lightweight CR like OverloadSignal) listing the overloaded nodes.

Your controller then evicts one running pod per overloaded node, records it (ConfigMap/CR), and that work item goes back to the queue. ScaledJob will automatically refill because ActiveJobs < desired.

6. Summary of the control logic

* **Admission loop (fast)**: every ~30s window → compute “available nodes” → let exactly that many pods run.
* **Placement**: nodeAffinity steers released pods to the **least-loaded nodes**; ties → random; hard rule: **one pod per node**.
* **Progression**: completion of a pod immediately **unblocks** one more queued pod.
* **Overload loop (slow)**: every 15 min → if a node looks overloaded by the 15-min average → **preempt** the pod there and **requeue** it.

7. Practical implications (I’m assuming these)

* We’ll use **KEDA ScaledJob** (or a small set of them) so KEDA can control **how many jobs run in parallel** (the “release” count = number of available nodes).
* We’ll still need a **small helper/controller** (or extend your existing cpu_scheduler.py) to:

  * Continuously compute **per-node load ranking** (for “least-loaded first”).
  * **Label** candidate nodes (so nodeAffinity can target them) and break ties randomly.
  * Perform the **15-minute overload eviction** (KEDA by itself doesn’t delete specific running pods on CPU; we’ll trigger that from this helper).
* Pods will have **requiredDuringScheduling** (or strong preferred) **nodeAffinity** toward the labeled least-loaded nodes, plus a guard to ensure **no two pods land on the same node** (e.g., via per-node “capacity 1” labels or a spread/anti-affinity trick).




